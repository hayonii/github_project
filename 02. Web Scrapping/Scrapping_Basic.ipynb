{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c7d1d7c7-931b-4dc8-991f-0f8273c57378",
   "metadata": {},
   "source": [
    "# request 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e10a71d-0f87-456f-91b2-19e372f97a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1855bfa-9267-428d-b8a9-2e97f76c1692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "URL = 'https://www.naver.com'\n",
    "response = requests.get(URL) # get 방식으로 요청\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7b560f-352b-4fa6-a3e1-b1e1d4df06d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dd1eb2-f231-4596-9bc4-c979859e4100",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'http://search.naver.com/search.naver'\n",
    "query = {'query' : 'python'}\n",
    "response = requests.get(URL, params=query)\n",
    "print(response.status_code)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81249d83-d8c3-4091-ae5f-83db1ff33ecd",
   "metadata": {},
   "source": [
    "# user-agent 값 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484198b3-54b5-4feb-a595-021dc82193cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "- 로봇이 아님을 나타내기 위해서 user-agent라는 값을 header에 넣어서 보냄\n",
    "- 직접적인 URL 주소로 요청 시 웹 사이트에서 웹 크롤링을 통해 접근한 것으로 감지하고 접속을 차단하게 됨\n",
    "- user-agent 헤더값을 포함하여 요청하면 브라우저를 통해 요청하는 것으로 인식되어 해결\n",
    "- 웹 브라우저 실행 -> F12 개발자 모드 진입 -> Console에 navigator.userAgent 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1773388b-00a5-45fd-8e25-d83f17c705bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "URL = 'http://www.google.com/search'\n",
    "params = {'q' : 'python'}\n",
    "headers = {'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36'}\n",
    "\n",
    "response = requests.get(URL, params=params, headers=headers)\n",
    "response.raise_for_status() # 응답코드가 200이 아니면 오류내고 멈춤\n",
    "\n",
    "result = response.text\n",
    "with open('mygoogle.html','w',encoding='utf-8') as f:\n",
    "    f.write(result)\n",
    "print('저장완료!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8566b65a-0543-43fe-8808-9add21c14efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "URL = 'http://www.google.com/search'\n",
    "params = {'q' : 'python'}\n",
    "headers = {'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36'}\n",
    "\n",
    "response = requests.get(URL, params=params)\n",
    "response.raise_for_status() # 응답코드가 200이 아니면 오류내고 멈춤\n",
    "\n",
    "result = response.text\n",
    "with open('mygoogle.html','w',encoding='utf-8') as f:\n",
    "    f.write(result)\n",
    "print('저장완료!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d2b946-1c90-485c-8d0e-1bb8bf6c0cec",
   "metadata": {},
   "source": [
    "## [실습] 네이터 데이터랩에서 실시간 인기 검색어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6f57e1-53a1-4e03-9fa3-67b0c6794520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get('https://datalab.naver.com')\n",
    "html_text = response.text\n",
    "\n",
    "temp = html_text.split('<em class=\"num\">1</em>')[1]\n",
    "temp = temp.split('<span class=\"title\">')[1]\n",
    "temp = temp.split('</span>')[0]\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e870aef6-6f12-4b4b-a04e-f37693b63de4",
   "metadata": {},
   "source": [
    "# BeautifulSoup 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5f96bf-08a1-4363-9c6f-a2fa22e88f60",
   "metadata": {},
   "source": [
    "## Parser 별 출력 결과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c444ccfa-420a-4cf0-a236-0a9b29ed12bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install html5lib\n",
    "# !pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceb543a-8487-42a5-8569-83e8e8586efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup('<a></p>', 'html.parser')\n",
    "print('html.parser')\n",
    "print(soup)\n",
    "print('-'*40)\n",
    "\n",
    "soup = BeautifulSoup('<a></p>', 'lxml')\n",
    "print('lxml')\n",
    "print(soup)\n",
    "print('-'*40)\n",
    "\n",
    "soup = BeautifulSoup('<a></p>', 'xml')\n",
    "print('xml')\n",
    "print(soup)\n",
    "print('-'*40)\n",
    "\n",
    "soup = BeautifulSoup('<a></p>', 'html5lib')\n",
    "print('html5lib')\n",
    "print(soup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5295b9-2a3d-4537-a8be-f52608339ca2",
   "metadata": {},
   "source": [
    "## 기본 사용법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175d7e75-08a5-4111-8d09-d21672e44429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "URL = 'https://ko.wikipedia.org/wiki/%EC%9B%B9_%ED%81%AC%EB%A1%A4%EB%9F%AC'\n",
    "response = requests.get(URL)\n",
    "\n",
    "# soup 객체 생성\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "# 태그를 이용한 접근\n",
    "print(soup.title)\n",
    "print(soup.footer.ul.li.text)\n",
    "\n",
    "# 태그와 속성을 이용한 접근\n",
    "print(soup.a) # 숲 객체에서 첫 번째로 만나는 a 태그 출력 \n",
    "# print(soup.a['id']) # 만약 속성이 존재하지 않으면 에러 발생\n",
    "print(soup.a['href'])\n",
    "\n",
    "# find() 함수를 이용한 태그 내의 다양한 속성을 이용한 접근 \n",
    "print(soup.find('a', attrs = {'title' : '구글봇'})) # 태그 중 title 속성의 값이 '구글봇' 인 데이터 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f675a60-c190-4b59-8ce4-0a93baf10d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "URL = 'https://ko.wikipedia.org/wiki/%EC%9B%B9_%ED%81%AC%EB%A1%A4%EB%9F%AC'\n",
    "response = requests.get(URL)\n",
    "\n",
    "# soup 객체 생성\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "# 태그를 이용한 접근\n",
    "print(soup.title)\n",
    "print(soup.footer.ul.li)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592b30b1-f3da-46a0-a572-f0546d8668ed",
   "metadata": {},
   "source": [
    "## 자식 노드들을 반복 가능한 객체로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc498f7-8014-41d4-9f38-7b50e09a65fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '''\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Web Scrapping</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p class=\"a\" align=\"center\">text1</p>\n",
    "        <p class=\"b\" align=\"center\">text2</p>\n",
    "        <p class=\"c\" align=\"center\">text3</p>\n",
    "        <div>\n",
    "            <img src=\"/source\" width=\"300\" height=\"200\">\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "contents = soup.find('body')\n",
    "# print(contents)\n",
    "for child in contents.children : \n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45422bb1-9cc6-4912-9e79-fef96beebb0c",
   "metadata": {},
   "source": [
    "## 자신을 포함한 노드들을 반복 가능한 객체로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08c7c21-ca3b-46e1-ba19-99c8f9c90e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '''\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Web Scrapping</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p class=\"a\" align=\"center\">text1</p>\n",
    "        <p class=\"b\" align=\"center\">text2</p>\n",
    "        <p class=\"c\" align=\"center\">text3</p>\n",
    "        <div>\n",
    "            <img src=\"/source\" width=\"300\" height=\"200\">\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "contents = soup.find('body')\n",
    "img_tag = contents.find('img')\n",
    "print(img_tag.parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e6b01f-1b2b-4d64-90ad-957143307514",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '''\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Web Scrapping</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p class=\"a\" align=\"center\">text1</p>\n",
    "        <p class=\"b\" align=\"center\">text2</p>\n",
    "        <p class=\"c\" align=\"center\">text3</p>\n",
    "        <div>\n",
    "            <img src=\"/source\" width=\"300\" height=\"200\">\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "contents = soup.find('body')\n",
    "\n",
    "img_tag = contents.find('img')\n",
    "print(img_tag.find_parent('body'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ee9303-75db-40d6-bc86-461d3ddac6ce",
   "metadata": {},
   "source": [
    "## 형제 노드 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb948df-4364-4fcb-bd65-22208e39d47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '''\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Web Scrapping</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p class=\"a\" align=\"center\">text1</p>\n",
    "        <p class=\"b\" align=\"center\">text2</p>\n",
    "        <p class=\"c\" align=\"center\">text3</p>\n",
    "        <div>\n",
    "            <img src=\"/source\" width=\"300\" height=\"200\">\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "p_tag = soup.find('p', attrs={'class': 'b'})\n",
    "print(p_tag)\n",
    "\n",
    "print('--바로 다음 형제 노드--')\n",
    "print(p_tag.find_next_sibling())\n",
    "print('--모든 다음 형제 노드--')\n",
    "print(p_tag.find_next_sibling())\n",
    "print('--바로 이전 형제 노드--')\n",
    "print(p_tag.find_previous_sibling())\n",
    "print('--모든 이전 형제 노드--')\n",
    "print(p_tag.find_previous_sibling())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a286985-c280-4097-8f0b-8c2c71c40c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get('http://www.naver.com')\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "print(soup.find('title'))\n",
    "print(soup.find('a'))\n",
    "print(soup.find(id='search')) # id 속성의 값이 'search'인 정보를 가져옴, soup.find(attrs={'id'='search'}) 동일"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3c6f6f-878f-4b71-b4e2-a6bea32c2f45",
   "metadata": {},
   "source": [
    "## 검색 : find_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7988160a-9d2b-4505-9baf-31a97378503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tags = soup.find_all('a', limit = 2)\n",
    "print(len(a_tags))\n",
    "print(a_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e767adbe-c3a3-4d20-8e63-1eff6c3698ee",
   "metadata": {},
   "source": [
    "[실습] 네이버 뉴스 페이지에서 언론사 목록 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad49195b-e73e-4c8f-bc17-16c2c8fca756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('http://news.naver.com')\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "result = soup.find_all('h4', attrs={'class':'channel'})\n",
    "# print(len(result))\n",
    "# print(result[0])\n",
    "# print(list(result[0].children))\n",
    "press_list = [list(tag.children)[0] for tag in result]\n",
    "print(press_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a4834a-8990-40f1-8739-c0f687a7ac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get('http://news.naver.com')\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "result = soup.find_all('div', attrs={'class': 'cjs_age_name'})\n",
    "press_list = [tag.text for tag in result]\n",
    "\n",
    "print(press_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173ea083-fefa-455b-91f8-a04e79e8500b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('http://www.tradecampus.com')\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "print(soup.select_one('div > a'))\n",
    "result = soup.select('div a')\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c4db05-d38a-496e-93bf-557732140522",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.select_one('body > div > div.wrapper.main_page > div.renew_main > div.col-12 > div > div.renew_main_notice > div > div > h3'))#.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e810825e-c37f-4c1d-a913-9e3c22ff050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tradecampus.com 메인 페이지 공지사항 2번째 항목 선택\n",
    "# :nth-child(#)을 지우면 해당 요소의 모든 요소를 가져온다.\n",
    "notice = soup.select('body > div > div.wrapper.main_page > div.renew_main > div.col-12 > div > div.renew_main_notice > div > ul > li:nth-child(2) > p > a')\n",
    "print(notice[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad90950-5818-48bd-b0b9-e33d5a810d3a",
   "metadata": {},
   "source": [
    "## 텍스트 가져오기 : text, get_text()\n",
    "- 검색 결과에서 태그를 제외한 텍스트만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd6ce1-a052-438b-ae5a-a1e49828328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고객센터 영역 텍스트 가져오기\n",
    "tag = soup.find('div', attrs = {'class':'serviceInfo'})\n",
    "# print(tag)\n",
    "print(tag.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45e6f70-5196-4200-9ae5-ed89d21c6cda",
   "metadata": {},
   "source": [
    "tag = soup.find('img', attrs = {'class':'mobile_icon black'})\n",
    "print(tag['src'])\n",
    "print(tag.get('src'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26fe855-9211-4f22-8190-f276b2ea2dfe",
   "metadata": {},
   "source": [
    "## 텍스트 가져오기 : string\n",
    "- 검색 결과에서 **태그안에 또 다른 태그가 없는 경우** 해당 내용을 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357e5093-ae3e-4451-b9c2-1dfd07c283fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = soup.find('div', attrs = {'class':'serviceInfo'})\n",
    "tag = tag.find('span')\n",
    "print(tag.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff4d39d-3a0d-45d8-8208-0758b8664c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "[실습] 네이버 웹툰 제목 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2617b4-7dba-4c58-af76-1ea8b0a5f11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실패\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = 'https://comic.naver.com/webtoon'\n",
    "res = requests.get(URL)\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "# webtoon_titles = soup.find_all('span',attrs={'class' : 'ContentTitle__title--e3qXt'})\n",
    "# print(len(webtoon_titles))\n",
    "webtoon_titles = soup.select_one('#container > div.component_wrap.type2 > div.WeekdayMainView__daily_all_wrap--UvRFc > div.WeekdayMainView__daily_all_item--DnTAH.WeekdayMainView__is_active--NSACG > ul > li:nth-child(1) > div > a > span > span')\n",
    "print(webtoon_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d5ac16-be83-4cb2-bde0-8abb985d526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b173198-1980-41bb-a54d-5edd9013b096",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4389cb12-c268-490f-b95f-806136e00903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "\n",
    "URL = 'https://comic.naver.com/webtoon'\n",
    "res = requests.get(URL)\n",
    "soup = BeautifulSoup(res.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5431f7-a5b5-46e2-801c-84d3c1fb4086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "\n",
    "URL = 'https://comic.naver.com/webtoon'\n",
    "driver.get(URL)\n",
    "time.sleep(4) # 동적으로 생성되는 페이지의 내용이 완성될 때까지 대기\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "# 요일별 전체 웹툰 CSS 선택자\n",
    "temp = soup.select_one('#container > div.component_wrap.type2 > div.WeekdayMainView__daily_all_wrap--UvRFc')\n",
    "# print(temp)\n",
    "# 요일별 div 태그 검색\n",
    "temp = temp.find_all('ul',attrs={'class':'WeekdayMainView__daily_list--R52q0'})\n",
    "print(len(temp))\n",
    "week = ['월', '화', '수', '목', '금', '토', '일']\n",
    "for i, w in enumerate(temp) : \n",
    "    print(f'===== {week[i]}요 웹툰 =====')\n",
    "    webtoon_list = w.find_all('li', attrs={'class':'DailyListItem__item--LP6_T'})\n",
    "    for webtoon in webtoon_list :\n",
    "        print(webtoon.find('span', attrs={'class':'text'}).text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77749ed0-87b0-4ade-82a4-1572736f7c2e",
   "metadata": {},
   "source": [
    "[실습] 메가박스 영화정보 사이트에서 영화 포스터 다운로드 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0e7cbf-4255-483e-9627-e2ac86bb5981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 테스트 : 박스 오피스 1위 영화 포스터 이미지 가져오기\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver import ChromeOptions, Chrome\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "URL = 'https://www.megabox.co.kr/movie'\n",
    "driver.get(URL)\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "poster_img = soup.select('#movieList > li > div.movie-list-info > img')\n",
    "print(len(poster_img))\n",
    "poster_img_src = poster_img[0].get('src') # get :특정 사진 가져오기\n",
    "\n",
    "import requests\n",
    "res = requests.get(poster_img_src)\n",
    "with open('poster.jpg', 'wb') as f:\n",
    "    f.write(res.content)\n",
    "print('End')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54389bad-8ebd-4e61-881e-dff7e228656e",
   "metadata": {},
   "source": [
    "[문제] 메가박스 영화 사이트에서 첫 페이지에 있는 모든 영화 포스트 이미지 수집하기\n",
    "- 메가박스 영화 사이트 첫 페이지에 있는 20개의 영화 포스트 이미지 수집\n",
    "- 현재 작업 디렉토리 밑에 'poster_img' 폴더가 없는 경우 폴더를 생성한다. (os 패키지 이용)\n",
    "- 저장되는 각 포스터 이미지의 파일 이름은 영화 제목으로 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14107d1-b1f1-447e-8b54-b7b008cd6941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 테스트 : 박스 오피스 1위 영화 포스터 이미지 가져오기\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver import ChromeOptions, Chrome\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "URL = 'https://www.megabox.co.kr/movie'\n",
    "driver.get(URL)\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "# 포스터 이미지를 가지고 있는 모든 img 태그를 검색\n",
    "poster_imgs = soup.find_all('img', attrs={'class' : 'poster lozad'})\n",
    "print(len(poster_imgs))\n",
    "\n",
    "# 이미지를 저장할 폴더 생성\n",
    "import os\n",
    "img_dir = './poster_img/'\n",
    "if not os.path.exists(img_dir) :\n",
    "    os.mkdir(img_dir)\n",
    "    print('폴더 생성 완료')\n",
    "else :\n",
    "    print('폴더가 존재함')\n",
    "\n",
    "for i, poster in enumerate(poster_imgs, 1):\n",
    "    title = poster.get('alt')\n",
    "    img_url = poster.get('src')\n",
    "\n",
    "    print(i, ':', img_url)\n",
    "    img_res = requests.get(img_url)\n",
    "    if ':' in title :\n",
    "        title = title.replace(':', ' ')\n",
    "\n",
    "    with open(img_dir+f'[{i}].{title}.jpg', 'wb') as f :\n",
    "        f.write(img_res.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f242edde-1674-41ea-a510-ebaf58d080b1",
   "metadata": {},
   "source": [
    "[실습] 네이버 뉴스 사이트에서 결제 관련 언론사별 랭킹뉴스 추출하기\n",
    "- 언론사 이름에 '경제' 단어가 포함된 언론사의 랭킹 뉴스만 추출하여 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc58349a-6443-4362-985b-588cdfa7416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#내 답변\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = 'https://news.naver.com/main/ranking/popularDay.naver'\n",
    "response = requests.get(URL)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "titles = soup.find_all('strong', attrs={'class' : 'rankingnews_name'})\n",
    "\n",
    "for title in titles:\n",
    "    if '경제' in title.text:\n",
    "        print(f\"언론사: {title.text}\")  \n",
    "        news_list = title.parent.parent.find('ul', attrs={'class': 'rankingnews_list'})\n",
    "        news_titles = news_list.find_all('div')\n",
    "        for news in news_titles:\n",
    "            news_title = news.find('a').text \n",
    "            print(f'{news_title}')\n",
    "        print()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c75eb97-9cc9-4b99-aeaa-8daa98360a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = 'https://news.naver.com/main/ranking/popularDay.naver'\n",
    "response = requests.get(URL)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "news_list = soup.find_all('div', attrs = {'class' : 'rankingnews_box'})\n",
    "print('등록 언론사 개수 :', len(news_list))\n",
    "\n",
    "for news in news_list :\n",
    "    press_title = news.find('strong').text\n",
    "    if '경제' in press_title :\n",
    "        print('언론사 : ', press_title)\n",
    "        press_news = news.find_all('div', attrs = {'class' : 'list_content'})\n",
    "        for i, ranking_news in enumerate(press_news, 1) :\n",
    "            print(f'{i} : {ranking_news.find(\"a\").get_text()}')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ee5e28-2cd4-4829-8bc9-e23b3c657ba8",
   "metadata": {},
   "source": [
    "# Selenium 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d41d5ce-1f72-40d8-84c8-8d268665a54f",
   "metadata": {},
   "source": [
    "## find_element() 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7b1346-b8d3-4ec0-b820-9a6e4fe4186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# 웹 드라이버 동적 다운로드 방식\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "\n",
    "# [참고] 로컬 컴퓨터에 설치되어있는 웹 드라이버 실행 방식\n",
    "# s = Service('d:\\DEV\\chromedriver\\chromedriver.exe')\n",
    "# driver = Chrome(service=s)\n",
    "\n",
    "driver.get('https://www.daum.net')\n",
    "ele = driver.find_element(by=By.LINK_TEXT, value='카페')\n",
    "print(type(ele))\n",
    "print(ele.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd77d7b-5ab7-4b5a-9476-abf6aacd0b79",
   "metadata": {},
   "source": [
    "## 이벤트 제어하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f745fca5-61e9-4454-b560-5f9eee340081",
   "metadata": {},
   "source": [
    "### click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d9d6c3-58f4-441e-801a-00300ba450e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "\n",
    "driver.get('http://www.naver.com')\n",
    "ele = driver.find_element(by=By.CSS_SELECTOR, value='#shortcutArea > ul > li:nth-child(5) > a')\n",
    "ele.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52a409c-d377-40a8-95df-a42656abe71e",
   "metadata": {},
   "source": [
    "### send_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d404dd66-df11-4c57-9cf4-b9426ed15f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys # 특수 키 사용을 위한 클래스\n",
    "\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "\n",
    "driver.get('http://www.naver.com')\n",
    "ele = driver.find_element(by=By.ID, value = 'query')\n",
    "ele.send_keys('python')\n",
    "ele.send_keys(Keys.ENTER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d28f3f-e674-48c5-8b1d-fd856b8ee118",
   "metadata": {},
   "source": [
    "[실습] 네이버 로그인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9412c0f2-43fe-40ec-bb75-386dbb741b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys # 특수 키 사용을 위한 클래스\n",
    "\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "\n",
    "driver.get('http://www.naver.com')\n",
    "ele = driver.find_element(by=By.CLASS_NAME, value = 'MyView-module__link_login___HpHMW')\n",
    "ele.click()\n",
    "my_id = 'gkdus8'\n",
    "my_pw = 'wlsk7515dus^^'\n",
    "\n",
    "# 로봇에 의해 클릭되지 못하도록 막았기 때문에 스크립트로 처리해야함\n",
    "# ele = driver.find_element(by=By.ID, value='id')\n",
    "# ele.send_keys(my_id)\n",
    "\n",
    "# ele = driver.find_element(by=By.ID, value='pw')\n",
    "# ele.send_keys(my_pw)\n",
    "\n",
    "driver.execute_script(f\"document.getElementById('id').value = '{my_id}'\")\n",
    "driver.execute_script(f\"document.getElementById('pw').value = '{my_pw}'\")\n",
    "\n",
    "# 로그인 상태 유지 체크박스 클릭\n",
    "driver.execute_script(f'document.getElementById(\"keep\").value = \"on\"')\n",
    "\n",
    "ele = driver.find_element(by=By.CLASS_NAME, value = 'btn_login')\n",
    "ele.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091f6d72-dfd7-4934-a44a-0f96b45904cf",
   "metadata": {},
   "source": [
    "## 웹 브라우저 자동 스크롤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a15437-1da8-4a9a-80d8-db3f50d01e03",
   "metadata": {},
   "source": [
    "### 구글에서 이미지 검색 후 검색 결과 6번 스크롤 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d33ebe-121e-4a6a-8811-20c43db308d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "SCROLL_PAUSE_TIME = 2\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "driver.get('https://www.google.com')\n",
    "ele = driver.find_element(by=By.CLASS_NAME, value = 'gLFyf')\n",
    "ele.send_keys('python')\n",
    "ele.submit()\n",
    "\n",
    "driver.find_element(By.LINK_TEXT, '이미지').click()\n",
    "\n",
    "# 페이지가 로드 될 때까지 기다림\n",
    "time.sleep(SCROLL_PAUSE_TIME)\n",
    "# 최초 스크롤 바의 높이값 읽기\n",
    "last_height = driver.execute_script('return document.body.scrollHeight')\n",
    "print('last height:', last_height)\n",
    "for i in range(6) :\n",
    "    # 윈도우의 스크롤 바를 0에서부터 가장 밑(scrollHeight)까지 이동\n",
    "    driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "    # 스크롤 바 이동으로 새로운 검색 결과가 로딩 후 변경된 새로운 스크롤 바의 높이 값 읽기\n",
    "    new_height = driver.execute_script('return document.body.scrollHeight')\n",
    "    print('new height:', new_height)\n",
    "    print('-'*30)\n",
    "\n",
    "    # 더이상 스크롤 될 페이지가 없을 경우 scrollHeight 값의 변화가 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9e9aed-3ee9-4abd-9dd2-34fbb53c7774",
   "metadata": {},
   "source": [
    "### 구글에서 이미지 검색 후 검색 결과 무한 스크롤 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da26bcd6-c13b-455b-bb95-1974933139b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "SCROLL_PAUSE_TIME = 2\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "driver.get('https://www.google.com')\n",
    "ele = driver.find_element(by=By.CLASS_NAME, value = 'gLFyf')\n",
    "ele.send_keys('python')\n",
    "ele.submit()\n",
    "\n",
    "driver.find_element(By.LINK_TEXT, '이미지').click()\n",
    "\n",
    "# 페이지가 로드 될 때까지 기다림\n",
    "time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "# 최초 스크롤 바의 높이값 읽기\n",
    "last_height = driver.execute_script('return document.body.scrollHeight')\n",
    "\n",
    "# 스크롤 횟수 저장하는 함수\n",
    "scroll_cnt = 0\n",
    "\n",
    "while True :\n",
    "    # 윈도우의 스크롤 바를 0에서부터 가장 밑(scrollHeight)까지 이동\n",
    "    driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')\n",
    "    scroll_cnt += 1\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "    # 스크롤 바 이동으로 새로운 검색 결과가 로딩 후 변경된 새로운 스크롤 바의 높이 값 읽기\n",
    "    new_height = driver.execute_script('return document.body.scrollHeight')\n",
    "    print(f'last height : {last_height}, new height : {new_height}, scroll count : {scroll_cnt}')\n",
    "    if last_height != new_height : # = 계속해서 new_height 값이 변경된다면\n",
    "        last_height = new_height\n",
    "    else : # 더 이상 new_height 값의 변동이 없다면\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390d3219-1b87-40dc-a61e-1371cae6a52a",
   "metadata": {},
   "source": [
    "### 구글에서 이미지 검색 후 썸네일 이미지 클릭하고 원본 이미지 주소 수집하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71748f39-9bcf-45ae-b213-739589b8d915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "SCROLL_PAUSE_TIME = 2 # 페이지가 로드될 때까지 기다리는 시간\n",
    "IMAGE_EXTRACT_NUM = 20 # 이미지 추출 개수\n",
    "SEARCH_KEYWORD = 'python'\n",
    "\n",
    "############## 웹 드라이버 실행 및 구글 접속 ##############\n",
    "\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "driver.get('https://www.google.com')\n",
    "\n",
    "############## 검색 ##############\n",
    "\n",
    "ele = driver.find_element(by=By.CLASS_NAME, value = 'gLFyf')\n",
    "ele.send_keys(SEARCH_KEYWORD)\n",
    "ele.submit()\n",
    "\n",
    "############## 이미지 검색 결과 페이지 이동 ##############\n",
    "\n",
    "driver.find_element(By.LINK_TEXT, '이미지').click()\n",
    "\n",
    "# 페이지가 로드 될 때까지 기다림\n",
    "time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "############## 이미지 검색 결과 페이지 스크롤 ##############\n",
    "\n",
    "# 최초 스크롤 바의 높이값 읽기\n",
    "last_height = driver.execute_script('return document.body.scrollHeight')\n",
    "\n",
    "# 스크롤 횟수 저장하는 함수\n",
    "scroll_cnt = 0\n",
    "\n",
    "while True :\n",
    "    # 윈도우의 스크롤 바를 0에서부터 가장 밑(scrollHeight)까지 이동\n",
    "    driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')\n",
    "    scroll_cnt += 1\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "    # 스크롤 바 이동으로 새로운 검색 결과가 로딩 후 변경된 새로운 스크롤 바의 높이 값 읽기\n",
    "    new_height = driver.execute_script('return document.body.scrollHeight')\n",
    "    print(f'last height : {last_height}, new height : {new_height}, scroll count : {scroll_cnt}')\n",
    "    if last_height != new_height : # = 계속해서 new_height 값이 변경된다면\n",
    "        last_height = new_height\n",
    "    else : # 더 이상 new_height 값의 변동이 없다면\n",
    "        break\n",
    "\n",
    "\n",
    "############## 이미지 선택 및 해당 이미지 src 추출 ##############\n",
    "\n",
    "imgs = driver.find_elements(By.CSS_SELECTOR, '#rso > div > div > div.wH6SXe.u32vCb > div > div > div > div.czzyk.XOEbc > h3 > a')\n",
    "print(len(imgs))\n",
    "img_src_list = []\n",
    "img_cnt = 0\n",
    "for img in imgs : \n",
    "    imgs.click()\n",
    "    try : \n",
    "        img_src = driver.find_element(By.XPATH, '//*[@id=\"Sva75c\"]/div[2]/div[2]/div[2]/div[2]/c-wiz/div/div/div/div/div[3]/div[1]/a/img[1]').get_attribute('src')\n",
    "        img_cnt += 1\n",
    "        print(f'[{img_cnt}].{img_src}')\n",
    "        if img_cnt == IMAGE_EXTRACT_NUM : break\n",
    "    except :\n",
    "        img_cnt -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740c7d77-d32a-4a1c-9267-cf828d0f9892",
   "metadata": {},
   "source": [
    "### 구글에서 이미지 검색 후 파일로 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77bd0216-8b08-43fa-97f4-6ad943ffa0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last height:5503, new height:10711, scroll count: 1\n",
      "last height:10711, new height:15671, scroll count: 2\n",
      "last height:15671, new height:16781, scroll count: 3\n",
      "last height:16781, new height:16781, scroll count: 4\n",
      "324\n",
      "[2].https://i.pinimg.com/736x/7d/80/98/7d8098c736328a3f352a5fdf5f0b5d0f.jpg\n",
      "[4].https://wallpapercave.com/wp/wp8765724.jpg\n",
      "[6].https://w0.peakpx.com/wallpaper/215/486/HD-wallpaper-pochacco-ice-cream-pochacco-pochacco-dog-sanrio.jpg\n",
      "[8].https://live.staticflickr.com/3742/11557599306_31e086073c_c.jpg\n",
      "[10].https://play-lh.googleusercontent.com/lq9_KxioRhUIJ59LalYzi6zb92oJyXTCDAwvRrBlUqIH45BF1dwaGwQDL1hECHFTGw=w2560-h1440-rw\n"
     ]
    },
    {
     "ename": "MaxRetryError",
     "evalue": "HTTPConnectionPool(host='localhost', port=64714): Max retries exceeded with url: /session/3b152a6349af71a6692531fdca386706/window (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000024622134F10>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:203\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     sock \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[0;32m    204\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport),\n\u001b[0;32m    205\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    206\u001b[0m         source_address\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_address,\n\u001b[0;32m    207\u001b[0m         socket_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options,\n\u001b[0;32m    208\u001b[0m     )\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 73\u001b[0m sock\u001b[38;5;241m.\u001b[39mconnect(sa)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:791\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 791\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    792\u001b[0m     conn,\n\u001b[0;32m    793\u001b[0m     method,\n\u001b[0;32m    794\u001b[0m     url,\n\u001b[0;32m    795\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    796\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    797\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    798\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    799\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    800\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    801\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    802\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    803\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    804\u001b[0m )\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:497\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     conn\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    498\u001b[0m         method,\n\u001b[0;32m    499\u001b[0m         url,\n\u001b[0;32m    500\u001b[0m         body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    501\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    502\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    503\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    504\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    505\u001b[0m         enforce_content_length\u001b[38;5;241m=\u001b[39menforce_content_length,\n\u001b[0;32m    506\u001b[0m     )\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:395\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendheaders()\n\u001b[0;32m    397\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1289\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_output(message_body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1048\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1048\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(msg)\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1051\u001b[0m \n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:986\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m--> 986\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:243\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_conn()\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:218\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    220\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m# Audit hooks are only available in Python 3.8+\u001b[39;00m\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x0000024622134F10>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 101\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     99\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(res\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m--> 101\u001b[0m driver\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:458\u001b[0m, in \u001b[0;36mWebDriver.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    451\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Closes the current window.\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \n\u001b[0;32m    453\u001b[0m \u001b[38;5;124;03m    :Usage:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;124;03m            driver.close()\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 458\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mCLOSE)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:345\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m    343\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[1;32m--> 345\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:302\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    300\u001b[0m trimmed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trim_large_entries(params)\n\u001b[0;32m    301\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, command_info[\u001b[38;5;241m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[1;32m--> 302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(command_info[\u001b[38;5;241m0\u001b[39m], url, body\u001b[38;5;241m=\u001b[39mdata)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:322\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    319\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[1;32m--> 322\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn\u001b[38;5;241m.\u001b[39mrequest(method, url, body\u001b[38;5;241m=\u001b[39mbody, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[0;32m    323\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\_request_methods.py:110\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[1;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[0;32m    107\u001b[0m     urlopen_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m body\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_url_methods:\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[0;32m    111\u001b[0m         method,\n\u001b[0;32m    112\u001b[0m         url,\n\u001b[0;32m    113\u001b[0m         fields\u001b[38;5;241m=\u001b[39mfields,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    114\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw,\n\u001b[0;32m    116\u001b[0m     )\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_body(\n\u001b[0;32m    119\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[0;32m    120\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\_request_methods.py:143\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_url\u001b[1;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fields:\n\u001b[0;32m    141\u001b[0m     url \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m urlencode(fields)\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\poolmanager.py:443\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    441\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 443\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, u\u001b[38;5;241m.\u001b[39mrequest_uri, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    445\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:875\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn:\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;66;03m# Try again\u001b[39;00m\n\u001b[0;32m    872\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    873\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[0;32m    874\u001b[0m     )\n\u001b[1;32m--> 875\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    876\u001b[0m         method,\n\u001b[0;32m    877\u001b[0m         url,\n\u001b[0;32m    878\u001b[0m         body,\n\u001b[0;32m    879\u001b[0m         headers,\n\u001b[0;32m    880\u001b[0m         retries,\n\u001b[0;32m    881\u001b[0m         redirect,\n\u001b[0;32m    882\u001b[0m         assert_same_host,\n\u001b[0;32m    883\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    884\u001b[0m         pool_timeout\u001b[38;5;241m=\u001b[39mpool_timeout,\n\u001b[0;32m    885\u001b[0m         release_conn\u001b[38;5;241m=\u001b[39mrelease_conn,\n\u001b[0;32m    886\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    887\u001b[0m         body_pos\u001b[38;5;241m=\u001b[39mbody_pos,\n\u001b[0;32m    888\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    889\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    890\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    891\u001b[0m     )\n\u001b[0;32m    893\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n\u001b[0;32m    894\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:875\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn:\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;66;03m# Try again\u001b[39;00m\n\u001b[0;32m    872\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    873\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[0;32m    874\u001b[0m     )\n\u001b[1;32m--> 875\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    876\u001b[0m         method,\n\u001b[0;32m    877\u001b[0m         url,\n\u001b[0;32m    878\u001b[0m         body,\n\u001b[0;32m    879\u001b[0m         headers,\n\u001b[0;32m    880\u001b[0m         retries,\n\u001b[0;32m    881\u001b[0m         redirect,\n\u001b[0;32m    882\u001b[0m         assert_same_host,\n\u001b[0;32m    883\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    884\u001b[0m         pool_timeout\u001b[38;5;241m=\u001b[39mpool_timeout,\n\u001b[0;32m    885\u001b[0m         release_conn\u001b[38;5;241m=\u001b[39mrelease_conn,\n\u001b[0;32m    886\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    887\u001b[0m         body_pos\u001b[38;5;241m=\u001b[39mbody_pos,\n\u001b[0;32m    888\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    889\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    890\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    891\u001b[0m     )\n\u001b[0;32m    893\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n\u001b[0;32m    894\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:875\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn:\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;66;03m# Try again\u001b[39;00m\n\u001b[0;32m    872\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    873\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[0;32m    874\u001b[0m     )\n\u001b[1;32m--> 875\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    876\u001b[0m         method,\n\u001b[0;32m    877\u001b[0m         url,\n\u001b[0;32m    878\u001b[0m         body,\n\u001b[0;32m    879\u001b[0m         headers,\n\u001b[0;32m    880\u001b[0m         retries,\n\u001b[0;32m    881\u001b[0m         redirect,\n\u001b[0;32m    882\u001b[0m         assert_same_host,\n\u001b[0;32m    883\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    884\u001b[0m         pool_timeout\u001b[38;5;241m=\u001b[39mpool_timeout,\n\u001b[0;32m    885\u001b[0m         release_conn\u001b[38;5;241m=\u001b[39mrelease_conn,\n\u001b[0;32m    886\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    887\u001b[0m         body_pos\u001b[38;5;241m=\u001b[39mbody_pos,\n\u001b[0;32m    888\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    889\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    890\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    891\u001b[0m     )\n\u001b[0;32m    893\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n\u001b[0;32m    894\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:845\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_e, (\u001b[38;5;167;01mOSError\u001b[39;00m, HTTPException)):\n\u001b[0;32m    843\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 845\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[0;32m    846\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39mnew_e, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    847\u001b[0m )\n\u001b[0;32m    848\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n\u001b[0;32m    850\u001b[0m \u001b[38;5;66;03m# Keep track of the error for the retry warning.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[0;32m    514\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    517\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_retry\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=64714): Max retries exceeded with url: /session/3b152a6349af71a6692531fdca386706/window (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000024622134F10>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "SCROLL_PAUSE_TIME = 2\n",
    "IMAGE_EXTRACT_NUM = 20 # 이미지 추출 개수\n",
    "SEARCH_KEYWORD = 'pochacco wallpaper'\n",
    "\n",
    "\n",
    "############ 웹 드라이버 실행 및 구글 접속 ############\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "driver.get('https://google.com')\n",
    "\n",
    "\n",
    "############ 검색 ############\n",
    "ele = driver.find_element(by=By.CLASS_NAME, value='gLFyf')\n",
    "ele.send_keys(SEARCH_KEYWORD)\n",
    "ele.submit()\n",
    "\n",
    "\n",
    "############ 이미지 검색결과 페이지 이동 ############\n",
    "driver.find_element(By.LINK_TEXT, '이미지').click()\n",
    "\n",
    "\n",
    "############ 페이지가 로드될때까지 기다림 ############\n",
    "time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "\n",
    "############ 이미지 검색결과 페이지 스크롤 ############\n",
    "last_height = driver.execute_script('return document.body.scrollHeight')\n",
    "\n",
    "\n",
    "############ 스크롤 횟수 저장 ############\n",
    "scroll_cnt = 0 \n",
    "\n",
    "\n",
    "while True:\n",
    "    # 윈도우의 스크롤 바를 0에서 부터 가장 밑(scrollHeight)까지 이동\n",
    "    driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')\n",
    "    scroll_cnt += 1\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "   #스크롤 바 이동으로 새로운 검색 결과가 로딩 후 변경된 새로운 스크롤 바의 높이 값 읽기\n",
    "    new_height = driver.execute_script('return document.body.scrollHeight')\n",
    "    print(f'last height:{last_height}, new height:{new_height}, scroll count: {scroll_cnt}')\n",
    "    if last_height != new_height: # 계속해서 new_height 값이 변경되면\n",
    "        last_height = new_height\n",
    "    else: # 더 이상 new_height 값의 변동이 없다면\n",
    "        break\n",
    "\n",
    "\n",
    "############ 이미지 선택 및 해당 이미지 src 추출 ############\n",
    "imgs = driver.find_elements(By.CSS_SELECTOR, '#rso > div > div > div.wH6SXe.u32vCb > div > div > div > div.czzyk.XOEbc > h3 > a')\n",
    "print(len(imgs))\n",
    "img_src_list = []\n",
    "img_cnt = 0\n",
    "for img in imgs:   \n",
    "    try:\n",
    "        img_cnt += 1\n",
    "        img.click()\n",
    "        time.sleep(2)\n",
    "        img_src = driver.find_element(By.CSS_SELECTOR, '#Sva75c > div.A8mJGd.NDuZHe.OGftbe-N7Eqid-H9tDt > div.LrPjRb > div.AQyBn > div.tvh9oe.BIB1wf > c-wiz > div > div > div > div > div.v6bUne > div.p7sI2.PUxBg > a > img.sFlh5c.pT0Scc.iPVvYb').get_attribute('src')\n",
    "        img_cnt += 1\n",
    "        print(f'[{img_cnt}].{img_src}')\n",
    "        if img_cnt == IMAGE_EXTRACT_NUM: break\n",
    "    except:\n",
    "        img_cnt -= 1\n",
    "\n",
    "\n",
    "############ 검색이미지 파일로 저장 ############\n",
    "import os\n",
    "import requests\n",
    "\n",
    "ts = time.localtime()\n",
    "path = 'c:/Temp/'\n",
    "now = '{}.{}.{}.{}.{}.{}'.format(ts.tm_year, ts.tm_mon, ts.tm_mday, ts.tm_hour, ts.tm_min, ts.tm_sec ) # 매개 변수에 넣는 값이 중괄호에 순서대로 들어간다\n",
    "dir = SEARCH_KEYWORD + '/' + now + '/'\n",
    "os.chdir(path)\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "\n",
    "file_no = 1\n",
    "os.chdir(path+dir)\n",
    "\n",
    "for url in img_src_list:\n",
    "    extension = url.split('.')[-1] # 원본 이미지에서 가져온 확장자\n",
    "    ext = '' # 최종적으로 사용할 이미지의 확장자\n",
    "    if extension in ['jpg', 'JPG', 'jpeg', 'JPEG', 'png', 'PNG','gif', 'GIF']:\n",
    "        ext = '.'+extension\n",
    "    else:\n",
    "        ext='.jpg'\n",
    "        \n",
    "    file_name = str(file_no)+'-'+SEARCH_KEYWORD+ext\n",
    "    file_no += 1\n",
    "    res = request.get(url)\n",
    "    with open(file_name, 'wb') as f:\n",
    "        f.write(res.content)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e830a64c-e8c1-4558-953a-014dc2c7c5c3",
   "metadata": {},
   "source": [
    "## select 태그 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcfcfb4-634b-4e24-8777-667a2c05136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import Select # select form 제어 클래스\n",
    "\n",
    "URL = 'https://www.selenium.dev/selenium/web/formPage.html'\n",
    "\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "driver.get(URL)\n",
    "\n",
    "ele = driver.find_element(By.NAME, 'selectomatic')\n",
    "select = Select(ele)\n",
    "\n",
    "# 인덱스 기준으로 선택하기\n",
    "# select.select_by_index(2)\n",
    "\n",
    "# 보여지는 선택값 텍스트로 선택하기\n",
    "# select.select_by_visible_text('Four')\n",
    "\n",
    "# option 요소의 값으로 선택하기\n",
    "# select.select_by_value('four')\n",
    "\n",
    "# select 태그에 onchange 속성이 있어서 Select 클래스를 사용할 수 없을 때\n",
    "driver.find_element(By.CSS_SELECTOR, 'option[value=\"four\"]').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3be4d6-2556-4c08-a48f-5170f2bcb359",
   "metadata": {},
   "source": [
    "[실습과제] yes24에서 파이썬 도서 검색하기\n",
    "- yes24 사이트에서 파이썬 도서 검색 -> 검색 결과를 120개 선택\n",
    "- 검색 결과에서 도서 평점이 9.6 이상인 도서 제목과 가격, 평점 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0c142e-e849-4f83-b20a-bfaa0d5f4d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import Select # select form 제어 클래스\n",
    "\n",
    "URL = 'https://www.yes24.com'\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "driver.get(URL)\n",
    "\n",
    "\n",
    "############ 검색 ############\n",
    "ele = driver.find_element(by=By.ID, value='query')\n",
    "ele.send_keys('파이썬')\n",
    "ele.send_keys(Keys.ENTER)\n",
    "\n",
    "driver.find_element(By.CSS_SELECTOR, '#stat_gb > option[value=\"120\"]').click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0138c213-ea97-426e-a6ab-13f969adaa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 가져오기\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "book_list = soup.find('ul', attrs={'id':'yesSchList'})\n",
    "books = book_list.find_all('li')\n",
    "print('검색 도서 권수 : ', len(books))\n",
    "\n",
    "count = 0\n",
    "for i, book in enumerate(books,1):\n",
    "    rating = book.find('span', attrs={'class':'rating_grade'})\n",
    "    if not rating : continue  # 평점이 없는 도서일 경우 이하 내용 실행 skip\n",
    "    rating = float(rating.find('em').get_text())\n",
    "    \n",
    "    if rating >= 9.6 :\n",
    "        count += 1\n",
    "        title = book.find('a', attrs = {'class' : 'gd_name'}).get_text()\n",
    "        price = book.find('strong', attrs = {'class' :'txt_num'}).get_text()\n",
    "        print(f'{count:03d} | {title} | {price} | {rating} 원')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea1adb2-96fd-41c1-9a16-37cbc5fee1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 가져오기\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "book_list = soup.find('ul', attrs={'id':'yesSchList'})\n",
    "books = book_list.find_all('li')\n",
    "print(books[-1])\n",
    "\n",
    "count = 0\n",
    "for i, book in enumerate(books,1):\n",
    "    rating = book.find('span', attrs={'class':'rating_grade'})\n",
    "    if not rating : continue  # 평점이 없는 도서일 경우 이하 내용 실행 skip\n",
    "    rating = float(rating.find('em').get_text())\n",
    "    if rating >= 9.6 :\n",
    "        count += 1\n",
    "        title = book.find('a', attrs = {'class' : 'gd_name'}).get_text()\n",
    "        price = book.find('strong', attrs = {'class' :'txt_num'}).get_text()\n",
    "        print(f'{count:03d} | {title} | {price} | {rating} 원')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c338a027-88db-48b0-813c-cac93c02646b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(books[0])\n",
    "print('='*40)\n",
    "print(books[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75683607-1b7d-4272-b651-d35c5fe12f12",
   "metadata": {},
   "source": [
    "[실습] 메가박스 영화 감상평 및 평점 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061d111c-8c81-4ec5-aad9-b202278e73f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "PAUSE_TIME = 3\n",
    "\n",
    "URL = 'https://www.megabox.co.kr/movie'\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "driver.get(URL)\n",
    "\n",
    "movie = driver.find_element(By.CSS_SELECTOR, '#movieList > li:nth-child(4) > div.movie-list-info > div.movie-score > a').send_keys(Keys.ENTER)\n",
    "time.sleep(PAUSE_TIME)\n",
    "driver.find_element(By.LINK_TEXT, '실관람평').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c95a89b9-6d39-48bd-b478-dfe693ec3240",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = []\n",
    "comments = []\n",
    "\n",
    "bs = BeautifulSoup(driver.page_source, 'lxml')\n",
    "result = bs.find_all('li', attrs={'class': 'type01 oneContentTag'})\n",
    "for c in result :\n",
    "    rating = int(c.find('div', attrs={'class' : 'story-point'}).text)\n",
    "    comment = c.find('div', attrs = {'class' : 'story-txt'}).text.strip()\n",
    "    ratings.append(rating)\n",
    "    comments.append(comment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "37e8f6f1-345c-4ef2-b930-04fd972526d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 페이지 수 :  2566\n",
      "1페이지 평점 및 리뷰 정보 수집\n",
      "2페이지 평점 및 리뷰 정보 수집\n",
      "3페이지 평점 및 리뷰 정보 수집\n",
      "4페이지 평점 및 리뷰 정보 수집\n",
      "5페이지 평점 및 리뷰 정보 수집\n",
      "6페이지 평점 및 리뷰 정보 수집\n",
      "7페이지 평점 및 리뷰 정보 수집\n",
      "8페이지 평점 및 리뷰 정보 수집\n",
      "9페이지 평점 및 리뷰 정보 수집\n",
      "10페이지 평점 및 리뷰 정보 수집\n",
      "11페이지 평점 및 리뷰 정보 수집\n",
      "12페이지 평점 및 리뷰 정보 수집\n",
      "13페이지 평점 및 리뷰 정보 수집\n",
      "14페이지 평점 및 리뷰 정보 수집\n",
      "15페이지 평점 및 리뷰 정보 수집\n",
      "16페이지 평점 및 리뷰 정보 수집\n",
      "17페이지 평점 및 리뷰 정보 수집\n",
      "18페이지 평점 및 리뷰 정보 수집\n",
      "19페이지 평점 및 리뷰 정보 수집\n",
      "20페이지 평점 및 리뷰 정보 수집\n"
     ]
    }
   ],
   "source": [
    "# 페이지 번호 클릭 테스트\n",
    "driver.find_element(By.CSS_SELECTOR, '#contentData > div > div.movie-idv-story > nav > a.control.last').click()\n",
    "time.sleep(PAUSE_TIME)\n",
    "total_page_num = int(driver.find_element(By.CSS_SELECTOR, '#contentData > div > div.movie-idv-story > nav > strong').text)\n",
    "print('전체 페이지 수 : ', total_page_num)\n",
    "# 첫 페이지 이동\n",
    "driver.find_element(By.CSS_SELECTOR, '#contentData > div > div.movie-idv-story > nav > a.control.first').click()\n",
    "time.sleep(PAUSE_TIME) \n",
    "\n",
    "ratings = []\n",
    "comments = []\n",
    "\n",
    "THE_LAST_PAGE = 20\n",
    "next_a_tag = 2 # 다음 클릭이 이뤄질 a 태그 순서(첫 페이지는 a태그가 아닌 strong 태그이기 때문에 첫 클릭이 이뤄질 태그 순서는 2부터 시작)\n",
    "\n",
    "for i in range(1, total_page_num+1):\n",
    "    if i > THE_LAST_PAGE : break\n",
    "    print(f'{i}페이지 평점 및 리뷰 정보 수집')\n",
    "    bs = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    result = bs.find_all('li', attrs={'class':'type01 oneContentTag'})\n",
    "    for c in result :\n",
    "        rating = int(c.find('div', attrs={'class' : 'story-point'}).text)\n",
    "        comment = c.find('div', attrs = {'class' : 'story-txt'}).text.strip()\n",
    "        ratings.append(rating)\n",
    "        comments.append(comment)\n",
    "\n",
    "    if not i%10 :# 10번째 페이지 평점 정보 수집이 끝나면...\n",
    "        driver.find_element(By.CSS_SELECTOR, '#contentData > div > div.movie-idv-story > nav > a.control.next').click() # 다음 10 페이지 보기 클릭\n",
    "        time.sleep(PAUSE_TIME)\n",
    "        next_a_tag = 4 # 다음 10페이지에서 그 다음 클릭이 이뤄질 a 태그의 순서는 4가 됨 (다음 클릭할 페이지 링크 앞에 <, <<, 첫페이지가 있어서...)\n",
    "        continue\n",
    "# 다음 페이지 번호 클릭\n",
    "    driver.find_element(By.CSS_SELECTOR, f'#contentData > div > div.movie-idv-story > nav > a:nth-child({next_a_tag})').click()\n",
    "    next_a_tag += 1\n",
    "    time.sleep(PAUSE_TIME)\n",
    "\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0c560c25-1fd1-4ab2-98fd-33069431bc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "관람객 평정 : 9.195점\n",
      "평점 저장 완료\n",
      "감상평 저장 완료\n"
     ]
    }
   ],
   "source": [
    "print(f'관람객 평정 : {sum(ratings)/len(ratings)}점')\n",
    "import csv\n",
    "import os\n",
    "\n",
    "with open('ratings.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for rating in ratings :\n",
    "        writer.writerow([rating])\n",
    "print('평점 저장 완료')\n",
    "\n",
    "with open('comment.txt', 'w', encoding='utf-8') as f :\n",
    "    for comment in comments :\n",
    "        f.write(comment+'\\n')\n",
    "print('감상평 저장 완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e27915-e0d4-42a4-9c0b-3168f4d366cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#마지막 페이지 이동\n",
    "driver.find_element(By.CSS_SELECTOR,'#contentData > div > div.movie-idv-story > nav > a.control.last').click()\n",
    "time.sleep(PAUSE_TIME)\n",
    "total_page_num = int(driver.find_element(By.CSS_SELECTOR,'#contentData > div > div.movie-idv-story > nav > strong').text)\n",
    "print('검색 페이지 수: ',total_page_num)\n",
    "#첫 페이지 이동\n",
    "driver.find_element(By.CSS_SELECTOR,'#contentData > div > div.movie-idv-story > nav > a.control.first').click()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
